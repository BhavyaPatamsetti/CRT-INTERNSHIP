{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load_Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport struct\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,ConfusionMatrixDisplay,confusion_matrix\nimport tensorflow as tf\nimport torch\nfrom torch import nn , optim\nfrom torchvision import datasets, transforms","metadata":{"id":"e566g9OdmLZ2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataLoading(object):\n    def __init__(self, training_images_filepath,training_labels_filepath,\n                 test_images_filepath, test_labels_filepath):\n        self.training_images_filepath = training_images_filepath\n        self.training_labels_filepath = training_labels_filepath\n        self.test_images_filepath = test_images_filepath\n        self.test_labels_filepath = test_labels_filepath\n    \n    def read_images_labels(self, images_filepath, labels_filepath):        \n        labels = []\n        with open(labels_filepath, 'rb') as file:\n            _,_ = struct.unpack('>2I',file.read(8))\n            labels = bytearray(file.read())        \n        with open(images_filepath, 'rb') as file:\n            _,size,rows,cols = struct.unpack(\">4I\", file.read(16))\n            image_data = bytearray(file.read())        \n        images = []\n        for i in range(size):\n            images.append([0] * rows * cols)\n        for i in range(size):\n            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n            img = img.reshape(28, 28)\n            images[i][:] = img            \n        \n        return images, labels\n\n\n    def load_data(self):\n        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n        return (x_train, y_train),(x_test, y_test)\n        \n\n    def prepare_data(self):\n      (x_train, y_train), (x_test, y_test) = self.load_data()\n      x_train = np.array(x_train).reshape(60000,784)\n      x_test = np.array(x_test).reshape(10000,784)\n      X_train = pd.DataFrame(x_train)\n      X_test = pd.DataFrame(x_test)\n      y_train = pd.DataFrame(np.array(y_train).flatten())\n      y_test = pd.DataFrame(np.array(y_test).flatten())\n      y_train.rename(columns={0:'target'},inplace =True)\n      y_test.rename(columns={0:'target'},inplace =True)\n      return (X_train,y_train),(X_test,y_test)\n    \n    def info(self):\n     (X_train, y_train), (X_test, y_test) = self.prepare_data()\n     print('Train_X_shape: {0} , Test_X_shape: {1}'.format(X_train.shape,X_test.shape))\n     print('-------------------------------------------------')\n     print('Train_y_shape: {0} , Test_y_shape: {1}'.format(y_train.shape,y_test.shape))\n     print('-------------------------------------------------')\n     print('5_rows_training: ')\n     print(X_train.head())\n     print('-------------------------------------------------')\n     print(y_train.head())\n     print('-------------------------------------------------')\n     print('5_rows_testing: ')\n     print(X_test.head())\n     print('-------------------------------------------------')\n     print(y_test.head())\n     print('-------------------------------------------------')\n     print('labels_numbers: ')\n     print(y_train.value_counts())\n     print('-------------------------------------------------')\n     print(y_test.value_counts())\n     \n    def show_image(self,number_row):\n      (X_train, y_train), (X_test, y_test) = self.prepare_data()\n      combine_X = pd.concat([X_train,X_test])\n      combine_y = pd.concat([y_train,y_test])\n      print('-------------------------------------------------')\n      print('label: {0}'.format(combine_y.iloc[number_row][0]))\n      print()\n      plt.imshow(combine_X.iloc[number_row,::].to_numpy().reshape(28,28),cmap = plt.cm.gray)\n    ","metadata":{"id":"wu6NtOPPexE7","execution":{"iopub.status.busy":"2023-05-10T07:52:55.152741Z","iopub.execute_input":"2023-05-10T07:52:55.153147Z","iopub.status.idle":"2023-05-10T07:52:55.173532Z","shell.execute_reply.started":"2023-05-10T07:52:55.153116Z","shell.execute_reply":"2023-05-10T07:52:55.172373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_images_filepath = '/kaggle/input/mnist-dataset/train-images.idx3-ubyte'\ntraining_labels_filepath = '/kaggle/input/mnist-dataset/train-labels.idx1-ubyte'\ntest_images_filepath = '/kaggle/input/mnist-dataset/t10k-images.idx3-ubyte'\ntest_labels_filepath = '/kaggle/input/mnist-dataset/t10k-labels.idx1-ubyte'\n\n# load MINST dataset\n\nmnist_dataloader = DataLoading(training_images_filepath, training_labels_filepath,\n                               test_images_filepath, test_labels_filepath)\n(X_train, y_train), (X_test, y_test) = mnist_dataloader.prepare_data()\n# info\nmnist_dataloader.info()\n\n# show image\nmnist_dataloader.show_image(10000)\n","metadata":{"id":"hg6wBIkUmjxW","outputId":"9bf713a1-b184-4492-eb48-2b616bb0ad04","execution":{"iopub.status.busy":"2023-05-10T07:53:01.365321Z","iopub.execute_input":"2023-05-10T07:53:01.366313Z","iopub.status.idle":"2023-05-10T07:53:11.413716Z","shell.execute_reply.started":"2023-05-10T07:53:01.366274Z","shell.execute_reply":"2023-05-10T07:53:11.412528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sickit-Learn","metadata":{"id":"NPJ8P1F5yJeQ"}},{"cell_type":"code","source":"class SickitAlgorithms(object):\n  def knn_model(slef,k_neighbors:int):\n    knn = KNeighborsClassifier(n_neighbors=k_neighbors)\n    knn.fit(X_train,y_train)\n    print()\n    print('----------------          KNN      -------------------')\n    print()\n    print('test_score: {0}'.format(np.round(knn.score(X_test,y_test)*100,2)))\n    print('---------------------------------------------------------')\n    print('classification_report: ')\n    print(classification_report(y_test,knn.predict(X_test)))\n    print('---------------------------------------------------------')\n    print('Confusion_matrix: ')\n    ConfusionMatrixDisplay(confusion_matrix(y_test,knn.predict(X_test))).plot()\n    plt.show()\n    print('---------------------------------------------------------')\n\n\n\n  def svc_model(slef,c:int):\n    svc = SVC(C=c)\n    svc.fit(X_train,y_train)\n    print()\n    print('----------------          SVC      -------------------')\n    print()\n    print('test_score: {0}'.format(np.round(svc.score(X_test,y_test)*100,2)))\n    print('---------------------------------------------------------')\n    print('classification_report: ')\n    print(classification_report(y_test,svc.predict(X_test)))\n    print('---------------------------------------------------------')\n    print('Confusion_matrix: ')\n    ConfusionMatrixDisplay(confusion_matrix(y_test,svc.predict(X_test))).plot()\n    plt.show()\n    print('---------------------------------------------------------')\n\n\n\n  def mlp_model(self,hidden_layer_size:tuple,active:str,optimizer:str):\n    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_size,activation = active,solver=optimizer)\n    mlp.fit(X_train,y_train)\n    print()\n    print('----------------          MLP      -------------------')\n    print()\n    print('test_score: {0}'.format(np.round(mlp.score(X_test,y_test)*100,2)))\n    print('---------------------------------------------------------')\n    print('classification_report: ')\n    print(classification_report(y_test,mlp.predict(X_test)))\n    print('---------------------------------------------------------')\n    print('Confusion_matrix: ')\n    ConfusionMatrixDisplay(confusion_matrix(y_test,mlp.predict(X_test))).plot()\n    plt.show()\n    print('---------------------------------------------------------')\n\n\n             \n ","metadata":{"id":"hWurB5BEzB2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"algorithm_training = SickitAlgorithms()\nalgorithm_training.knn_model(5)\nalgorithm_training.svc_model(100)\nalgorithm_training.mlp_model((10,5),'tanh','sgd')","metadata":{"id":"fJaF61OMeVGI","outputId":"e1d64523-8546-41ab-b623-5d001fdb6dcf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TensorFlow","metadata":{"id":"BNsgSMnk7BhD"}},{"cell_type":"code","source":"# Define the model architecture\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Flatten(), # input\n  tf.keras.layers.Dense(128, activation='relu'),# 1 hidden layer with 128 neurons\n  tf.keras.layers.Dense(10) # output after 10 epoch\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', # using adam \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # sparse categorical loss\n              metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10)\n\n# Evaluate the model on the test set\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\nprint('----------------------------------')\nprint('Test accuracy:', test_acc)\nprint('Test loss:',test_loss)","metadata":{"id":"uSOl0W_a7Fvi","outputId":"6970ae1e-8039-4497-af60-389ac6ac6f38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch","metadata":{"id":"urN8MYfVmzcJ"}},{"cell_type":"code","source":"# Define the neural network architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 512) # size , number of neurons\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)\n    \n    def forward(self, x):\n        x = x.view(-1, 784) # reshaping input \n        x = nn.functional.tanh(self.fc1(x)) # pass input to relu/tanh activation function\n        x = nn.functional.tanh(self.fc2(x)) # pass hidden to relu/tanh activation function\n        x = self.fc3(x) \n        return nn.functional.log_softmax(x, dim=1) # output that will be from [0----->9] after doing log_softmax \n           \n# Define the training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data) # forward propagation\n        loss = nn.functional.cross_entropy(output, target)\n        loss.backward() # backward propagation\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n# Define the test function\ndef test(model, device, test_loader):\n    model.eval() # evaluation mode\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            # compares the predictions to the true labels,\n            # and adds the number of correct predictions to the correct variable to get percentage .\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100 * correct / len(test_loader.dataset)))\n\n","metadata":{"id":"t8_CL-0w12xl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the device (use GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the MNIST dataset and apply transformations\ntrain_dataset = datasets.MNIST('data', train=True, download=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))\n                             ]))\ntest_dataset = datasets.MNIST('data', train=False, download=True,\n                            transform=transforms.Compose([\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.1307,), (0.3081,))\n                            ]))\n\n# Set up the data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64 ,shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=True)\n\n# Instantiate the neural network and move it to the device\nmodel = Net().to(device)\n\n# Set up the optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n# Train and test the model for 10 epochs\nfor epoch in range(1, 11):\n    train(model, device, train_loader, optimizer, epoch)\n\ntest(model, device, test_loader)","metadata":{"id":"2Ugp0gGs2Kui","outputId":"df533cd6-124c-4bcc-a8b2-b948efb0bc23"},"execution_count":null,"outputs":[]}]}